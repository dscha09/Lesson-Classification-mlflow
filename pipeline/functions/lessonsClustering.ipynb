{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Test Machine\\Anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "C:\\Users\\Test Machine\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n",
      "C:\\Users\\Test Machine\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "C:\\Users\\Test Machine\\Anaconda3\\lib\\site-packages\\nltk\\lm\\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "C:\\Users\\Test Machine\\Anaconda3\\lib\\site-packages\\pandasticsearch\\queries.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class Query(collections.MutableSequence):\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import ClippedCorpus\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils\n",
    "import pyLDAvis.gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "from DataFunctions import ElasticFunctions as ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    python lessonsClustering.py \\\n",
    "        --fine_tuning \\\n",
    "        --renew_tfidf \\\n",
    "        --number_of_topics 14 \\\n",
    "        --alpha 0.91 \\\n",
    "        --beta 0.91\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "# Arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--fine_tuning\", dest=\"fine_tuning\", default=False, required=False, action='store_true')\n",
    "    parser.add_argument(\"--renew_tfidf\", dest=\"renew_tfidf\", default=False, required=False, action='store_true')\n",
    "    parser.add_argument(\"--renew_positions\", dest=\"renew_positions\", default=False, required=False, action='store_true')\n",
    "    parser.add_argument(\"--number_of_topics\", dest=\"number_of_topics\", default=14, required=True, type=int)\n",
    "    parser.add_argument(\"--alpha\", dest=\"alpha\", default=0.91, required=True, type=float)\n",
    "    parser.add_argument(\"--beta\", dest=\"beta\", default=0.91, required=True, type=float)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Credentials\n",
    "    credentials = {\n",
    "        # \"ip_and_port\": \"52.163.240.214:9200\",\n",
    "        \"ip_and_port\": \"52.230.8.63:9200\",\n",
    "        \"username\": \"elastic\",\n",
    "        \"password\": \"Welcometoerni!\"\n",
    "    }\n",
    "\n",
    "    prodCredentials = {\n",
    "        \"ip_and_port\": \"52.163.240.214:9200\",\n",
    "        # \"ip_and_port\": \"52.230.8.63:9200\",\n",
    "        \"username\": \"elastic\",\n",
    "        \"password\": \"Welcometoerni!\"\n",
    "    }\n",
    "\n",
    "# Get lessons data from database\n",
    "\n",
    "    df = ef.getLessons(credentials)\n",
    "\n",
    "# Pre Processing\n",
    "    lessonsData = df[df['isLesson'] == True]\n",
    "    lessonsData = lessonsData[lessonsData['summary'] == lessonsData['summary']]\n",
    "    lessonsData = lessonsData[0:20]\n",
    "    raw_paragraphs = lessonsData['paragraph']\n",
    "    urls = lessonsData['urlToFile']\n",
    "    raw_sentences = raw_paragraphs\n",
    "    ids = lessonsData['_id']\n",
    "    referenceIds = lessonsData['referenceId']\n",
    "    sentences = [line.split(' ') for line in raw_sentences]    \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'äô', 'äù', 'äì'])\n",
    "    words_to_remove = ['iii', 'project']\n",
    "\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def remove_words(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in words_to_remove] for doc in texts]\n",
    "\n",
    "    def remove_word_length_2(texts):\n",
    "        allSentences = []\n",
    "        for doc in texts:\n",
    "            newWords = []\n",
    "            for word in doc:\n",
    "                if len(word) > 2:\n",
    "                    newWords.append(word)\n",
    "            allSentences.append(newWords)\n",
    "        return allSentences\n",
    "\n",
    "    def replace_adb_special_characters(texts):\n",
    "        return [[word.replace('‚Äôs', \"'s \").replace('O‚ÄôSmach', \"0\").replace('äù', \"\").replace('äô', \"\").replace('äì', \"\") for word in doc] for doc in texts]\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    sentences = replace_adb_special_characters(sentences)\n",
    "    data_words_nostops = remove_stopwords(sentences)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = []\n",
    "    for paragraph in data_words_nostops:\n",
    "        lemmatized_output.append([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in paragraph])\n",
    "    sentences = remove_words(lemmatized_output)\n",
    "    sentences_no_length_2 = remove_word_length_2(sentences)\n",
    "    sentences = sentences_no_length_2\n",
    "\n",
    "    id2word = corpora.Dictionary(sentences)\n",
    "    texts = sentences\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "        lda_model = LdaMulticore(corpus=corpus,\n",
    "                                id2word=id2word,\n",
    "                                num_topics=k, \n",
    "                                random_state=100,\n",
    "                                chunksize=100,\n",
    "                                passes=10,\n",
    "                                alpha=a,\n",
    "                                eta=b,\n",
    "                                per_word_topics=True)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=sentences, dictionary=id2word, coherence='c_v')\n",
    "        return coherence_model_lda.get_coherence()\n",
    "\n",
    "# Fine Tuning\n",
    "    if args.fine_tuning:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import tqdm\n",
    "        grid = {}\n",
    "        grid['Validation_Set'] = {}\n",
    "\n",
    "        # Topics range\n",
    "        min_topics = 2\n",
    "        max_topics = 15\n",
    "        step_size = 1\n",
    "        topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "        # Alpha parameter\n",
    "        alpha = list(np.arange(0.01, 1, 0.3))\n",
    "        alpha.append('symmetric')\n",
    "        alpha.append('asymmetric')\n",
    "\n",
    "        # Beta parameter\n",
    "        beta = list(np.arange(0.01, 1, 0.3))\n",
    "        beta.append('symmetric')\n",
    "\n",
    "        # Validation sets\n",
    "        num_of_docs = len(corpus)\n",
    "        corpus_sets = [\n",
    "        #                ClippedCorpus(corpus, int(num_of_docs*0.25)), \n",
    "                    ClippedCorpus(corpus, int(num_of_docs*0.5)), \n",
    "        #                ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "                    corpus\n",
    "                    ]\n",
    "        corpus_title = [\n",
    "                        '25% Corpus'\n",
    "                        '50% Corpus', \n",
    "                        '75% Corpus'\n",
    "                        '100% Corpus'\n",
    "                    ]\n",
    "        model_results = {'Validation_Set': [],\n",
    "                        'Topics': [],\n",
    "                        'Alpha': [],\n",
    "                        'Beta': [],\n",
    "                        'Coherence': []\n",
    "                        }\n",
    "        if 1 == 1:\n",
    "            pbar = tqdm.tqdm(total=2000)\n",
    "            for i in range(len(corpus_sets)):\n",
    "                for k in topics_range:\n",
    "                    for a in alpha:\n",
    "                        for b in beta:\n",
    "                            cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                        k=k, a=a, b=b)\n",
    "                            model_results['Validation_Set'].append(corpus_title[i])\n",
    "                            model_results['Topics'].append(k)\n",
    "                            model_results['Alpha'].append(a)\n",
    "                            model_results['Beta'].append(b)\n",
    "                            model_results['Coherence'].append(cv)\n",
    "                            pbar.update(1)\n",
    "                            # TODO: Save to elasticsearch\n",
    "                            # pd.DataFrame(model_results).to_csv(oldDataFileName + '-fine-tuning-.csv', index=False)\n",
    "\n",
    "# Run LDA model\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                        id2word=id2word,\n",
    "                        num_topics=args.number_of_topics, \n",
    "                        random_state=200,\n",
    "                        chunksize=100,\n",
    "                        passes=10,\n",
    "                        alpha=args.alpha,\n",
    "                        eta=args.beta,\n",
    "                        per_word_topics=True)\n",
    "\n",
    "# Save TFIDF model\n",
    "    if args.renew_tfidf:\n",
    "        tfidf = TfidfModel(corpus, smartirs='ntc')\n",
    "        tfidf_corpus = []\n",
    "        for doc in corpus:\n",
    "            tfidf_corpus.append(tfidf[doc])\n",
    "        tfidf_mat = matutils.corpus2dense(tfidf_corpus, num_terms=len(id2word.token2id))\n",
    "        tfidf_mat_transpose = tfidf_mat.transpose()\n",
    "        dfTFIDF=pd.DataFrame(data=tfidf_mat_transpose[0:,0:],\n",
    "                            index=[i for i in range(tfidf_mat_transpose.shape[0])],\n",
    "                            columns=[''+str(i) for i in range(tfidf_mat_transpose.shape[1])])\n",
    "        dfTFIDF['id'] = ids.tolist()\n",
    "\n",
    "        ef.deleteIndex(credentials, \"tfidf\")\n",
    "        ef.saveTFIDF(credentials, dfTFIDF)\n",
    "\n",
    "# Keyword weights\n",
    "    x=lda_model.show_topics(num_topics=args.number_of_topics, num_words=50,formatted=False)\n",
    "    keywordWeights = []\n",
    "    topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "    for tp in x:\n",
    "        words = []\n",
    "        weights = []\n",
    "        for pair in tp[1]:\n",
    "            words.append(pair[0])\n",
    "            weights.append(int(pair[1]*10000))\n",
    "        keywordWeights.append(weights)\n",
    "\n",
    "# Top topics per paragraph\n",
    "    df = pd.DataFrame()\n",
    "    df['referenceId'] = referenceIds\n",
    "    df['paragraph'] = raw_paragraphs\n",
    "    topicNumbers = []\n",
    "    for c in range(len(corpus)):\n",
    "        maxProbability = 0\n",
    "        indexOfMax = 0\n",
    "        topTopics = []\n",
    "        topTopicProbabilities = []\n",
    "        lda_model.get_document_topics(corpus[c])\n",
    "        for topicNumber in lda_model.get_document_topics(corpus[c]):\n",
    "            topTopics.append(topicNumber[0])\n",
    "            topTopicProbabilities.append(topicNumber[1])\n",
    "        topTopicsSorted = [x for _, x in sorted(zip(topTopicProbabilities, topTopics), reverse=True)]\n",
    "        topicNumbers.append(topTopicsSorted)\n",
    "    df['topTopics'] = topicNumbers\n",
    "\n",
    "# Most probable topic per paragraph\n",
    "    topTopics = []\n",
    "    for index, row in df.iterrows():\n",
    "        if(row['topTopics']):\n",
    "            topTopics.append(row['topTopics'][0])\n",
    "        else:\n",
    "            topTopics.append(-1)\n",
    "    df['topic'] = topTopics\n",
    "\n",
    "# Frequencies of topic keywords and number of PCRs per topic\n",
    "    topics = pd.DataFrame()\n",
    "    topicKeywords = []\n",
    "    allKeywords = []\n",
    "    topicIds = []\n",
    "    for topic, words in topics_words:\n",
    "        allKeywords.append(words)\n",
    "        topicIds.append(topic)\n",
    "    topics['key'] = topicIds\n",
    "    topics['keywords'] = allKeywords\n",
    "    topics['oldFrequencies'] = [ [0] * len(keywords) for keywords in allKeywords]\n",
    "    topics['numberOfLessons'] = 0\n",
    "    topics['PCRs'] = [[]  for i in range(len(topics))]\n",
    "    topics['numberOfPCRs'] = 0\n",
    "\n",
    "    for sentenceTopicNumbers, sentenceURL in zip(topicNumbers, urls):\n",
    "        for topicNumber in sentenceTopicNumbers:\n",
    "            topics.at[topicNumber, 'numberOfLessons'] = topics.at[topicNumber, 'numberOfLessons'] + 1\n",
    "            topics.at[topicNumber, 'PCRs'].append(sentenceURL)\n",
    "    for index, row in topics.iterrows():\n",
    "        topics.at[index, 'numberOfPCRs'] = len(set(topics.at[index, 'PCRs']))\n",
    "    topics = topics.drop(columns=['PCRs'])\n",
    "\n",
    "# Frequencies of words per sentence per topic\n",
    "    topics['oldFrequencies'] = [ [0] * len(keywords) for keywords in allKeywords]\n",
    "    for index, row in topics.iterrows():\n",
    "        topicNumber = topics.at[index, 'key']\n",
    "        topicKeywords = topics.at[index, 'keywords']\n",
    "        topicKeywordsFrequencies = topics.at[index, 'oldFrequencies']\n",
    "        for sentence, sentenceTopicNumbers in zip(sentences, topicNumbers):\n",
    "            for sentenceTopicNumber in sentenceTopicNumbers:\n",
    "                if topicNumber == sentenceTopicNumber:\n",
    "                    for word in sentence:\n",
    "                        if word in topicKeywords:\n",
    "                            indexOfWord = topicKeywords.index(word)\n",
    "                            topicKeywordsFrequencies[indexOfWord] = topicKeywordsFrequencies[indexOfWord] + 1\n",
    "        topics.at[index, 'oldFrequencies'] = topicKeywordsFrequencies\n",
    "    topics['frequencies'] = keywordWeights\n",
    "    \n",
    "# Top word per topic\n",
    "    topicTopWords = []\n",
    "    for index, row in topics.iterrows():\n",
    "        topicTopWords.append(row['keywords'][0])\n",
    "    topics['topWord'] = topicTopWords\n",
    "\n",
    "# Adjacent topics\n",
    "    # pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "    topics['x'] = 1.0\n",
    "    topics['y'] = 1.0\n",
    "    for topic, x in zip(list(vis.topic_coordinates.index), list(vis.topic_coordinates.x)):\n",
    "        topics.at[topic, 'x'] = float(x)\n",
    "    for topic, y in zip(list(vis.topic_coordinates.index), list(vis.topic_coordinates.y)):\n",
    "        topics.at[topic, 'y'] = float(y)\n",
    "\n",
    "    import math  \n",
    "    def calculateDistance(x1,y1,x2,y2):  \n",
    "        dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)  \n",
    "        return dist  \n",
    "\n",
    "    distanceMatrix = []\n",
    "    allDistances = []\n",
    "    c1 = 0\n",
    "    topicsX = topics['x'].tolist()\n",
    "    topicsY = topics['y'].tolist()\n",
    "    for tx1, ty1 in zip(topicsX, topicsY):\n",
    "        distances = []\n",
    "        for tx2, ty2 in zip(topicsX, topicsY):\n",
    "            distance = calculateDistance(tx1, ty1, tx2, ty2)\n",
    "            if not distance:\n",
    "                distance = 999\n",
    "            else:\n",
    "                allDistances.append(distance)\n",
    "            distances.append(distance)\n",
    "        distanceMatrix.append(distances)\n",
    "        c1 = c1 + 1\n",
    "    \n",
    "    percentile20 = np.percentile(allDistances, 20)\n",
    "    numberOfAdjacent = 0\n",
    "    numberOfNodes = len(distanceMatrix)\n",
    "    allAdjacentTopics = []\n",
    "    for distances in distanceMatrix:\n",
    "        adjacentTopics = []\n",
    "        for index, distance in zip(range(len(distances)), distances):\n",
    "            if distance <= percentile20:\n",
    "                adjacentTopics.append(index)\n",
    "        allAdjacentTopics.append(adjacentTopics)\n",
    "        numberOfAdjacent = numberOfAdjacent + len(adjacentTopics)\n",
    "    numberOfAdjacent = numberOfAdjacent/2\n",
    "    pairs = []\n",
    "    for index, adjacentTopicList in zip(range(len(allAdjacentTopics)), allAdjacentTopics):\n",
    "        for adjacentTopic in adjacentTopicList:\n",
    "            pairs.append(sorted([index, adjacentTopic]))\n",
    "    pairs.sort()\n",
    "    dedupedPairs = list(pairs for pairs, _ in itertools.groupby(pairs))\n",
    "    topWordPairs = []\n",
    "    for pair in dedupedPairs:\n",
    "        topWordPairs.append([topicTopWords[pair[0]], topicTopWords[pair[1]]])\n",
    "    topics['adjacentTopics'] = allAdjacentTopics\n",
    "\n",
    "# Save topics data\n",
    "    ef.deleteIndex(credentials, \"topics\")\n",
    "    ef.saveTopics(credentials, topics)\n",
    "\n",
    "# Lesson strength\n",
    "    maxLessonStrength = topics['numberOfPCRs'].sum()\n",
    "    lessonStrengths = []\n",
    "    for index, row in df.iterrows():\n",
    "        topicNumbers = row['topTopics']\n",
    "        lessonStrength = 0\n",
    "        for topicNumber in topicNumbers:\n",
    "            lessonStrength = lessonStrength + topics.at[topicNumber, 'numberOfPCRs']\n",
    "        lessonStrengths.append(lessonStrength/maxLessonStrength)\n",
    "    df['lessonStrength'] = lessonStrengths\n",
    "    # dfLessonStrength = df[['_id', 'Lesson Strength','topic', 'topTopics']]\n",
    "\n",
    "# TODO: Save lesson topics, lesson strengths\n",
    "    # ef.updateLessons(credentials, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
